```markdown
# üõí E-commerce Anomaly Detection Data Ingestion Pipeline

‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠ Data Pipeline ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ **PySpark** ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Transaction ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö (Ingestion) ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• **PostgreSQL** ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô 2 ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏´‡∏•‡∏±‡∏Å ‡∏Ñ‡∏∑‡∏≠ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ò‡∏∏‡∏£‡∏Å‡∏£‡∏£‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (`transactions`) ‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ò‡∏∏‡∏£‡∏Å‡∏£‡∏£‡∏°‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥ (`anomalies`)

---

## üõ†Ô∏è Tech Stack & Prerequisites

* **Orchestration/Execution:** Docker, Docker Compose
* **Big Data Processing:** Apache Spark (PySpark)
* **Database:** PostgreSQL
* **Driver:** PostgreSQL JDBC Driver

---

## üì¶ ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå

```

.
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ consumed\_transactions.csv  \# ‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ó‡∏î‡∏™‡∏≠‡∏ö
‚îú‚îÄ‚îÄ pyspark\_writer\_template.py \# ‡πÇ‡∏Ñ‡πâ‡∏î PySpark ‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Ingestion
‚îî‚îÄ‚îÄ README.md                  \# ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ô‡∏µ‡πâ

````

---

## üöÄ 1. ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ô‡∏£‡∏∞‡∏ö‡∏ö

### 1.1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ô Docker Containers

‡πÉ‡∏ä‡πâ `docker-compose` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ô PostgreSQL ‡πÅ‡∏•‡∏∞ Spark Submit Container:

```bash
docker compose up -d
````

### 1.2. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Database Schema (PostgreSQL)

‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ DBeaver ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏≠‡∏∑‡πà‡∏ô ‡πÜ ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö PostgreSQL ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô:

**Host:** `localhost:5432` | **DB:** `ecomm_fraud_db` | **User/Pass:** `postgres/123`

**‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á SQL ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á:**

```sql
-- 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÅ‡∏°‡πà: transactions
CREATE TABLE transactions (
    transaction_id INT PRIMARY KEY,
    user_id INT NOT NULL,
    amount DECIMAL(10, 2) NOT NULL,
    transaction_timestamp TIMESTAMP WITHOUT TIME ZONE NOT NULL,
    is_anomalous INT
);

-- 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏•‡∏π‡∏Å: anomalies (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ Foreign Key ‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà transactions)
CREATE TABLE anomalies (
    anomaly_id SERIAL PRIMARY KEY,
    transaction_id INT UNIQUE NOT NULL,
    user_id INT NOT NULL,
    amount DECIMAL(10, 2) NOT NULL,
    transaction_timestamp TIMESTAMP WITHOUT TIME ZONE NOT NULL,
    FOREIGN KEY (transaction_id) REFERENCES transactions(transaction_id)
);
```

### 1.3. ‡∏£‡∏±‡∏ô PySpark Ingestion Job

‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á `docker exec` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏±‡πà‡∏á‡πÉ‡∏´‡πâ Spark Submit Container ‡∏£‡∏±‡∏ô PySpark Job:

```bash
docker exec -it pyspark_submit_container spark-submit /app/pyspark_writer_template.py /app/consumed_transactions.csv
```

-----

## üìù 2. ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (Verification)

### 2.1. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ DBeaver Session (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç)

‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å DBeaver ‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Caching ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏°‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏´‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏á‡∏ñ‡∏π‡∏Å Commit ‡πÉ‡∏´‡∏°‡πà **‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á Disconnect ‡πÅ‡∏•‡∏∞ Reconnect** ‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

### 2.2. ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏•‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ô‡∏ã‡πâ‡∏≥ (End-to-End Test)

‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏ö (Append Test) ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ô‡∏ã‡πâ‡∏≥ 2 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á:

1.  **‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡πÉ‡∏ô DBeaver):**
    ```sql
    TRUNCATE TABLE anomalies RESTART IDENTITY;
    TRUNCATE TABLE transactions RESTART IDENTITY CASCADE; 
    ```
2.  **‡∏£‡∏±‡∏ô Job 2 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á:** (‡∏ó‡∏≥‡∏ã‡πâ‡∏≥‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠ 1.3 ‡∏™‡∏≠‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á)
3.  **‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏£‡∏ß‡∏°:** (‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å Disconnect/Reconnect DBeaver ‡πÅ‡∏•‡πâ‡∏ß)
    ```sql
    SELECT count(*) FROM transactions; 
    -- ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á: 3600 (1800 rows x 2 runs)

    SELECT count(*) FROM anomalies;
    -- ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á: 76 (38 anomalies x 2 runs)
    ```

-----

## ‚ö†Ô∏è 3. ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (Troubleshooting)

‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£ **Silent Rollback** ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å Spark Job ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏™‡∏±‡πà‡∏á Commit:

  * **‡∏õ‡∏±‡∏ç‡∏´‡∏≤:** Log ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡πÅ‡∏ï‡πà DBeaver ‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (COUNT = 0)
  * **‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏:** Transaction Rollback ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å Job ‡∏à‡∏ö‡∏•‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ñ‡∏∂‡∏á‡∏à‡∏∏‡∏î `spark.stop()`.
  * **‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:** ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏Ñ‡πâ‡∏î **`pyspark_writer_template.py`** ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ **`spark.stop()`** ‡πÉ‡∏ô `finally` block ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£ Commit/Cleanup ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á

-----

*‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ PySpark ‡πÅ‡∏•‡∏∞ PostgreSQL ‡∏†‡∏≤‡∏¢‡πÉ‡∏ï‡πâ‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏° Docker*

```
```
